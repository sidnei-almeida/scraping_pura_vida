#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸŒ¿ SCRAPING PURA VIDA - Interface Bonita
=======================================
Script principal padronizado para scraping incremental dos produtos Pura Vida
"""
import os
import sys
import time
import glob
import json
from datetime import datetime
import pandas as pd
from config.scraper import extract_nutritional_info
from config.url_collector import collect_product_urls

# ================= CORES ANSI =================
class Cores:
    RESET = '\033[0m'
    BOLD = '\033[1m'
    VERDE = '\033[92m'
    AZUL = '\033[94m'
    AMARELO = '\033[93m'
    VERMELHO = '\033[91m'
    CIANO = '\033[96m'
    MAGENTA = '\033[95m'
    BRANCO = '\033[97m'

# =============== UTILITÃRIAS ================
def limpar_terminal():
    os.system('clear' if os.name == 'posix' else 'cls')

def mostrar_banner():
    banner = f"""
{Cores.CIANO}{Cores.BOLD}
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                ğŸŒ¿ SCRAPING PURA VIDA                        â•‘
â•‘                                                            â•‘
â•‘    Coleta de Dados Nutricionais de Produtos v1.0           â•‘
â•‘                                                            â•‘
â•‘  ğŸ“Š ExtraÃ§Ã£o incremental de produtos e sabores             â•‘
â•‘  ğŸ¯ ExportaÃ§Ã£o incremental para CSV                        â•‘
â•‘  ğŸ“ VisualizaÃ§Ã£o e limpeza de arquivos                     â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
{Cores.RESET}"""
    print(banner)

def mostrar_barra_progresso(texto: str, duracao: float = 2.0):
    print(f"\n{Cores.AMARELO}â³ {texto}...{Cores.RESET}")
    barra_tamanho = 40
    for i in range(barra_tamanho + 1):
        progresso = i / barra_tamanho
        barra = "â–ˆ" * i + "â–‘" * (barra_tamanho - i)
        porcentagem = int(progresso * 100)
        print(f"\r{Cores.VERDE}[{barra}] {porcentagem}%{Cores.RESET}", end="", flush=True)
        time.sleep(duracao / barra_tamanho)
    print()

def mostrar_menu():
    menu = f"""
{Cores.AZUL}{Cores.BOLD}â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• MENU PRINCIPAL â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•{Cores.RESET}

{Cores.VERDE}ğŸš€ OPERAÃ‡Ã•ES:{Cores.RESET}
  {Cores.AMARELO}1.{Cores.RESET} ğŸ”— {Cores.BRANCO}Coletar URLs dos Produtos{Cores.RESET} - Busca todas as URLs
  {Cores.AMARELO}2.{Cores.RESET} ğŸŒ {Cores.BRANCO}Coletar Dados de Todas as URLs{Cores.RESET} - Scraping incremental
  {Cores.AMARELO}3.{Cores.RESET} ğŸ¯ {Cores.BRANCO}Coleta Completa (URLs + Dados){Cores.RESET} - Tudo de uma vez
  {Cores.AMARELO}4.{Cores.RESET} ğŸ§ª {Cores.BRANCO}Teste: Coleta de 10 Produtos{Cores.RESET} - Teste rÃ¡pido

{Cores.VERDE}ğŸ“ GERENCIAR DADOS:{Cores.RESET}
  {Cores.AMARELO}5.{Cores.RESET} ğŸ“‹ {Cores.BRANCO}Ver Arquivos{Cores.RESET} - Lista arquivos gerados
  {Cores.AMARELO}6.{Cores.RESET} ğŸ—‘ï¸  {Cores.BRANCO}Limpar Dados{Cores.RESET} - Remove arquivos antigos

{Cores.VERDE}â„¹ï¸  INFORMAÃ‡Ã•ES:{Cores.RESET}
  {Cores.AMARELO}7.{Cores.RESET} ğŸ“– {Cores.BRANCO}Sobre o Programa{Cores.RESET} - InformaÃ§Ãµes e estatÃ­sticas
  {Cores.AMARELO}8.{Cores.RESET} âŒ {Cores.BRANCO}Sair{Cores.RESET} - Encerrar programa

{Cores.AZUL}â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•{Cores.RESET}
"""
    print(menu)

def obter_escolha() -> str:
    try:
        escolha = input(f"{Cores.MAGENTA}ğŸ‘‰ Digite sua opÃ§Ã£o (1-8): {Cores.RESET}").strip()
        return escolha
    except KeyboardInterrupt:
        print(f"\n\n{Cores.AMARELO}âš ï¸  Programa interrompido pelo usuÃ¡rio{Cores.RESET}")
        sys.exit(0)

# ========== FUNÃ‡Ã•ES DO PROJETO =============
def save_incremental(df_novo, csv_file='dados/produtos.csv'):
    if os.path.exists(csv_file):
        df_existente = pd.read_csv(csv_file)
        df_total = pd.concat([df_existente, df_novo], ignore_index=True)
        df_total = df_total.drop_duplicates(subset=['NOME_PRODUTO'], keep='last')
    else:
        df_total = df_novo
    df_total.to_csv(csv_file, index=False)
    return len(df_novo), len(df_total)

def executar_scraping_incremental():
    print(f"\n{Cores.CIANO}{Cores.BOLD}ğŸŒ COLETANDO DADOS DE TODAS AS URLs{Cores.RESET}")
    print(f"{Cores.AZUL}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”{Cores.RESET}")
    if not os.path.exists('dados/product_urls.json'):
        print(f"{Cores.VERMELHO}âŒ Arquivo de URLs nÃ£o encontrado!{Cores.RESET}")
        return
    with open('dados/product_urls.json', 'r', encoding='utf-8') as f:
        urls = json.load(f)
    total_urls = len(urls)
    print(f"\n{Cores.VERDE}ğŸ”— Total de URLs para processar: {total_urls}{Cores.RESET}")
    confirmar = input(f"\n{Cores.MAGENTA}ğŸ¤” Iniciar scraping incremental? (s/N): {Cores.RESET}").lower()
    if confirmar not in ['s', 'sim', 'y', 'yes']:
        print(f"{Cores.AMARELO}â­ï¸  OperaÃ§Ã£o cancelada{Cores.RESET}")
        return
    mostrar_barra_progresso("Preparando scraping", 1.0)
    inicio = time.time()
    for i, url in enumerate(urls, 1):
        print(f"{Cores.AMARELO}({i}/{total_urls}) {Cores.BRANCO}Processando:{Cores.RESET} {url}")
        df = extract_nutritional_info(url)
        nome_produto = df['NOME_PRODUTO'].iloc[0] if df is not None and not df.empty and 'NOME_PRODUTO' in df.columns else 'Desconhecido'
        if df is not None:
            novos, total = save_incremental(df)
            print(f"{Cores.VERDE}âœ” {novos} linhas adicionadas. Total no CSV: {total}{Cores.RESET}")
        else:
            print(f"{Cores.VERMELHO}âš  Nenhum dado extraÃ­do para: {url}{Cores.RESET}")
        # Feedback visual
        perc = (i / total_urls) * 100
        tempo_decorrido = time.time() - inicio
        tempo_restante = (tempo_decorrido / i) * (total_urls - i) if i > 0 else 0
        barra = 'â–ˆ' * int(perc // 2) + 'â–‘' * (50 - int(perc // 2))
        print(f"{Cores.CIANO}[{barra}] {perc:.1f}%{Cores.RESET}")
        print(f"{Cores.AZUL}Produto: {Cores.BRANCO}{nome_produto}{Cores.RESET}")
        print(f"{Cores.AMARELO}Restantes: {total_urls - i}{Cores.RESET} | {Cores.VERDE}Tempo decorrido: {tempo_decorrido:.1f}s{Cores.RESET} | {Cores.CIANO}Est. restante: {tempo_restante:.1f}s{Cores.RESET}")
        print(f"{Cores.AZUL}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”{Cores.RESET}")
        time.sleep(0.1)
    print(f"\n{Cores.VERDE}ğŸ Coleta finalizada!{Cores.RESET}")

def listar_arquivos_gerados():
    print(f"\n{Cores.CIANO}{Cores.BOLD}ğŸ“‹ ARQUIVOS GERADOS{Cores.RESET}")
    print(f"{Cores.AZUL}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”{Cores.RESET}")
    pasta_dados = "dados"
    extensao = "*.csv"
    if not os.path.exists(pasta_dados):
        print(f"{Cores.AMARELO}ğŸ“ Pasta '{pasta_dados}' nÃ£o encontrada{Cores.RESET}")
        return
    arquivos = glob.glob(f"{pasta_dados}/{extensao}")
    if not arquivos:
        print(f"{Cores.AMARELO}ğŸ“„ Nenhum arquivo encontrado em '{pasta_dados}'{Cores.RESET}")
        return
    print(f"\n{Cores.VERDE}ğŸ“Š Total de arquivos: {len(arquivos)}{Cores.RESET}\n")
    for i, arquivo in enumerate(sorted(arquivos, reverse=True), 1):
        nome_arquivo = os.path.basename(arquivo)
        tamanho = os.path.getsize(arquivo)
        data_modificacao = datetime.fromtimestamp(os.path.getmtime(arquivo))
        if tamanho < 1024:
            tamanho_str = f"{tamanho} B"
        elif tamanho < 1024 * 1024:
            tamanho_str = f"{tamanho / 1024:.1f} KB"
        else:
            tamanho_str = f"{tamanho / (1024 * 1024):.1f} MB"
        print(f"{Cores.AMARELO}{i:2d}.{Cores.RESET} {Cores.BRANCO}{nome_arquivo}{Cores.RESET}")
        print(f"     ğŸ“… {data_modificacao.strftime('%d/%m/%Y %H:%M:%S')}")
        print(f"     ğŸ“ {tamanho_str}")
        print()

def limpar_dados_antigos():
    print(f"\n{Cores.CIANO}{Cores.BOLD}ğŸ—‘ï¸  LIMPAR DADOS ANTIGOS{Cores.RESET}")
    print(f"{Cores.AZUL}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”{Cores.RESET}")
    pasta_dados = "dados"
    extensao = "*.csv"
    if not os.path.exists(pasta_dados):
        print(f"{Cores.AMARELO}ğŸ“ Pasta '{pasta_dados}' nÃ£o encontrada{Cores.RESET}")
        return
    arquivos = glob.glob(f"{pasta_dados}/{extensao}")
    if not arquivos:
        print(f"{Cores.VERDE}âœ… Nenhum arquivo para limpar{Cores.RESET}")
        return
    print(f"\n{Cores.AMARELO}âš ï¸  ATENÃ‡ÃƒO:{Cores.RESET}")
    print(f"   â€¢ SerÃ£o removidos {Cores.VERMELHO}{len(arquivos)} arquivos{Cores.RESET}")
    print(f"   â€¢ Esta aÃ§Ã£o {Cores.VERMELHO}NÃƒO PODE ser desfeita{Cores.RESET}")
    confirmar = input(f"\n{Cores.MAGENTA}ğŸ¤” Tem certeza? Digite 'CONFIRMAR' para prosseguir: {Cores.RESET}")
    if confirmar == "CONFIRMAR":
        try:
            for arquivo in arquivos:
                os.remove(arquivo)
            print(f"\n{Cores.VERDE}âœ… {len(arquivos)} arquivos removidos com sucesso!{Cores.RESET}")
        except Exception as e:
            print(f"\n{Cores.VERMELHO}âŒ Erro ao remover arquivos: {e}{Cores.RESET}")
    else:
        print(f"{Cores.AMARELO}â­ï¸  OperaÃ§Ã£o cancelada{Cores.RESET}")

def mostrar_sobre():
    print(f"\n{Cores.CIANO}{Cores.BOLD}ğŸ“– SOBRE O PROGRAMA{Cores.RESET}")
    print(f"{Cores.AZUL}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”{Cores.RESET}")
    print(f"\n{Cores.VERDE}ğŸŒ¿ Scraping Pura Vida v1.0{Cores.RESET}")
    print(f"{Cores.BRANCO}Ferramenta para coleta automÃ¡tica de dados nutricionais dos produtos Pura Vida{Cores.RESET}")
    print(f"\n{Cores.VERDE}ğŸ“Š Funcionalidades:{Cores.RESET}")
    print(f"   â€¢ Coleta incremental de dados de mÃºltiplos produtos e sabores")
    print(f"   â€¢ ExportaÃ§Ã£o incremental para CSV")
    print(f"   â€¢ Interface visual padronizada e amigÃ¡vel")
    print(f"\n{Cores.VERDE}ğŸ’¾ Arquivos gerados:{Cores.RESET}")
    print(f"   â€¢ {Cores.AMARELO}dados/product_urls.json{Cores.RESET} - Lista de URLs dos produtos")
    print(f"   â€¢ {Cores.AMARELO}dados/produtos.csv{Cores.RESET} - Dados nutricionais extraÃ­dos")
    print(f"\n{Cores.VERDE}ğŸ› ï¸  Tecnologias utilizadas:{Cores.RESET}")
    print(f"   â€¢ Python 3.x")
    print(f"   â€¢ Selenium WebDriver + BeautifulSoup + Pandas")
    print(f"   â€¢ Chrome/Firefox em modo headless")
    input(f"\n{Cores.MAGENTA}Pressione ENTER para voltar ao menu...{Cores.RESET}")

def pausar():
    input(f"\n{Cores.MAGENTA}Pressione ENTER para continuar...{Cores.RESET}")

def executar_coleta_urls():
    print(f"\n{Cores.CIANO}{Cores.BOLD}ğŸ”— COLETANDO URLs DOS PRODUTOS{Cores.RESET}")
    print(f"{Cores.AZUL}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”{Cores.RESET}")
    confirmar = input(f"\n{Cores.MAGENTA}ğŸ¤” Iniciar coleta de URLs? (s/N): {Cores.RESET}").lower()
    if confirmar not in ['s', 'sim', 'y', 'yes']:
        print(f"{Cores.AMARELO}â­ï¸  OperaÃ§Ã£o cancelada{Cores.RESET}")
        return
    mostrar_barra_progresso("Preparando coleta de URLs", 1.0)
    try:
        collect_product_urls()
        print(f"{Cores.VERDE}âœ… Coleta de URLs concluÃ­da!{Cores.RESET}")
    except Exception as e:
        print(f"{Cores.VERMELHO}âŒ Erro durante execuÃ§Ã£o: {e}{Cores.RESET}")

def executar_coleta_completa():
    print(f"\n{Cores.CIANO}{Cores.BOLD}ğŸ¯ COLETA COMPLETA (URLs + Dados){Cores.RESET}")
    print(f"{Cores.AZUL}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”{Cores.RESET}")
    confirmar = input(f"\n{Cores.MAGENTA}ğŸ¤” Iniciar coleta completa? (s/N): {Cores.RESET}").lower()
    if confirmar not in ['s', 'sim', 'y', 'yes']:
        print(f"{Cores.AMARELO}â­ï¸  OperaÃ§Ã£o cancelada{Cores.RESET}")
        return
    mostrar_barra_progresso("Coletando URLs", 1.0)
    try:
        collect_product_urls()
        print(f"{Cores.VERDE}âœ… URLs coletadas!{Cores.RESET}")
    except Exception as e:
        print(f"{Cores.VERMELHO}âŒ Erro ao coletar URLs: {e}{Cores.RESET}")
        return
    mostrar_barra_progresso("Coletando dados nutricionais", 1.0)
    executar_scraping_incremental()

def coletar_10_urls_primeira_pagina():
    from config.browser import get_browser_driver
    base_url = "https://www.corpoevidasuplementos.com.br"
    search_url = f"{base_url}/advanced_search_result.php?keywords=Pura%20Vida"
    driver, browser_name = get_browser_driver(headless=True)
    driver.get(search_url)
    import time
    time.sleep(2)
    html = driver.page_source
    from bs4 import BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')
    links = soup.select('a.produto')
    urls = []
    for link in links:
        href = link.get('href')
        if href and href.startswith(base_url):
            urls.append(href)
        if len(urls) >= 10:
            break
    driver.quit()
    return urls

def executar_teste_10_produtos():
    print(f"\n{Cores.CIANO}{Cores.BOLD}ğŸ§ª TESTE: Coleta de 10 Produtos (Primeira PÃ¡gina){Cores.RESET}")
    print(f"{Cores.AZUL}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”{Cores.RESET}")
    confirmar = input(f"\n{Cores.MAGENTA}ğŸ¤” Iniciar teste de 10 produtos? (s/N): {Cores.RESET}").lower()
    if confirmar not in ['s', 'sim', 'y', 'yes']:
        print(f"{Cores.AMARELO}â­ï¸  OperaÃ§Ã£o cancelada{Cores.RESET}")
        return
    mostrar_barra_progresso("Coletando 10 URLs da primeira pÃ¡gina", 1.0)
    try:
        urls_teste = coletar_10_urls_primeira_pagina()
        with open('dados/teste.json', 'w', encoding='utf-8') as f:
            json.dump(urls_teste, f, ensure_ascii=False, indent=2)
        print(f"{Cores.VERDE}âœ… 10 URLs salvas em dados/teste.json!{Cores.RESET}")
    except Exception as e:
        print(f"{Cores.VERMELHO}âŒ Erro ao coletar URLs: {e}{Cores.RESET}")
        return
    mostrar_barra_progresso("Coletando dados dos 10 produtos", 1.0)
    import pandas as pd
    df_total = pd.DataFrame()
    for i, url in enumerate(urls_teste, 1):
        print(f"{Cores.AMARELO}({i}/10) {Cores.BRANCO}Processando:{Cores.RESET} {url}")
        df = extract_nutritional_info(url)
        if df is not None:
            df_total = pd.concat([df_total, df], ignore_index=True)
            print(f"{Cores.VERDE}âœ” Dados extraÃ­dos para a URL!{Cores.RESET}")
        else:
            print(f"{Cores.VERMELHO}âš  Nenhum dado extraÃ­do para: {url}{Cores.RESET}")
        mostrar_barra_progresso(f"Progresso teste: {i}/10", 0.2)
    df_total.to_csv('dados/teste.csv', index=False)
    print(f"\n{Cores.VERDE}ğŸ Teste finalizado! Dados salvos em dados/teste.csv{Cores.RESET}")

def main():
    while True:
        limpar_terminal()
        mostrar_banner()
        mostrar_menu()
        escolha = obter_escolha()
        if escolha == '1':
            executar_coleta_urls()
        elif escolha == '2':
            executar_scraping_incremental()
        elif escolha == '3':
            executar_coleta_completa()
        elif escolha == '4':
            executar_teste_10_produtos()
        elif escolha == '5':
            listar_arquivos_gerados()
        elif escolha == '6':
            limpar_dados_antigos()
        elif escolha == '7':
            mostrar_sobre()
        elif escolha == '8':
            print(f"\n{Cores.VERDE}âœ¨ Obrigado por usar o Scraping Pura Vida! AtÃ© logo!{Cores.RESET}")
            sys.exit(0)
        else:
            print(f"\n{Cores.VERMELHO}âŒ OpÃ§Ã£o invÃ¡lida! Tente novamente.{Cores.RESET}")
        pausar()

if __name__ == "__main__":
    main() 